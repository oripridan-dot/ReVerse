# -*- coding: utf-8 -*-
"""src/reverse/main_enterprise.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gM_gkLf2M4o0P7bBTwCRKNOJrXAtjqh
"""

"""
ReVerse Enterprise Edition
Production-ready multi-agent orchestration platform
"""

import os
import json
import asyncio
import logging
import inspect
from typing import Optional, Dict, Any, List, Union, Tuple
from datetime import datetime, timedelta, timezone
from contextlib import asynccontextmanager
from functools import lru_cache
import hashlib
import secrets

from src.reverse import agents
from fastapi import FastAPI, HTTPException, Depends, Security, status, Request, BackgroundTasks
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.exceptions import RequestValidationError
from pydantic import BaseModel, Field as PydanticField, field_validator, ConfigDict
try:  # Prefer modern settings interface (Pydantic v2)
    from pydantic_settings import BaseSettings
except ImportError:
    try:  # Pydantic v1 compatibility
        from pydantic import BaseSettings  # type: ignore[assignment]
    except ImportError:
        class BaseSettings(BaseModel):  # type: ignore[misc]
            """Minimal fallback implementation for settings management."""

            model_config = ConfigDict(arbitrary_types_allowed=True)


def Field(*args, **kwargs):  # type: ignore[override]
    """Compatibility wrapper supporting deprecated ``regex`` argument."""
    if "regex" in kwargs and "pattern" not in kwargs:
        kwargs["pattern"] = kwargs.pop("regex")
    return PydanticField(*args, **kwargs)
from pydantic.types import SecretStr
import socketio
import redis.asyncio as aioredis
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import sentry_sdk
from sentry_sdk.integrations.asgi import SentryAsgiMiddleware
from jose import jwt, JWTError
import uvicorn
from circuitbreaker import circuit
from tenacity import retry, stop_after_attempt, wait_exponential
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import Column, String, DateTime, JSON, Boolean, Integer, select, update, event
from sqlalchemy.orm import declarative_base
from sqlalchemy.ext.mutable import MutableDict

# --- Configuration Management ---
class Settings(BaseSettings):
    """Application settings with validation"""

    # Application
    app_name: str = "ReVerse Enterprise"
    app_version: str = "4.0.0"
    debug: bool = False
    environment: str = Field("development", regex="^(development|staging|production)$")

    # Security
    secret_key: SecretStr = Field(
        default=SecretStr("dev-secret-key-change-me-please-1234567890"),
        min_length=32
    )
    jwt_algorithm: str = "HS256"
    jwt_expiration_hours: int = 24
    api_key_header: str = "X-API-Key"
    allowed_hosts: List[str] = ["*"]
    cors_origins: List[str] = ["*"]

    # Database
    database_url: str = Field(
        default="postgresql+asyncpg://postgres:postgres@localhost:5432/reverse",
        regex="^postgresql"
    )
    database_pool_size: int = 20
    database_max_overflow: int = 40

    # Redis
    redis_url: str = "redis://localhost:6379"
    redis_pool_size: int = 10
    redis_decode_responses: bool = True

    # Rate Limiting
    rate_limit_per_minute: int = 60
    rate_limit_burst: int = 10

    # Monitoring
    sentry_dsn: Optional[str] = None
    metrics_enabled: bool = True
    log_level: str = "INFO"

    # Agent Configuration
    max_agents: int = 10
    agent_timeout: int = 30
    agent_retry_attempts: int = 3

    # OpenAI/Ollama
    openai_api_key: Optional[SecretStr] = None
    ollama_host: str = "http://localhost:11434"
    default_model: str = "phi3:mini"

    model_config = ConfigDict(env_file=".env", case_sensitive=False)

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance"""
    return Settings()

# --- Database Models ---
Base = declarative_base()


def utc_now() -> datetime:
    """Return the current UTC time as a timezone-aware datetime."""

    return datetime.now(timezone.utc)

class Job(Base):
    """Job tracking model"""
    __tablename__ = "jobs"

    id = Column(String, primary_key=True)
    user_id = Column(String, nullable=False, index=True)
    status = Column(String, default="pending")
    created_at = Column(DateTime(timezone=True), default=utc_now)
    updated_at = Column(DateTime(timezone=True), default=utc_now, onupdate=utc_now)
    completed_at = Column(DateTime(timezone=True), nullable=True)
    payload = Column(JSON)
    result = Column(JSON)
    error = Column(String, nullable=True)
    metadata_json = Column("metadata", MutableDict.as_mutable(JSON), default=dict)


@event.listens_for(Job, "init", propagate=True)
def _init_job_metadata(target, args, kwargs):
    """Ensure each ``Job`` instance starts with its own metadata dictionary."""
    if target.metadata_json is None:
        target.metadata_json = {}

class AuditLog(Base):
    """Audit trail for compliance"""
    __tablename__ = "audit_logs"

    id = Column(Integer, primary_key=True, autoincrement=True)
    timestamp = Column(DateTime(timezone=True), default=utc_now, index=True)
    user_id = Column(String, nullable=True)
    action = Column(String, nullable=False)
    resource = Column(String, nullable=True)
    details = Column(JSON)
    ip_address = Column(String, nullable=True)
    user_agent = Column(String, nullable=True)

# --- Metrics ---
request_counter = Counter('reverse_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
request_duration = Histogram('reverse_request_duration_seconds', 'Request duration', ['method', 'endpoint'])
active_connections = Gauge('reverse_active_connections', 'Active WebSocket connections')
job_counter = Counter('reverse_jobs_total', 'Total jobs by status', ['status'])
agent_execution_time = Histogram('reverse_agent_execution_seconds', 'Agent execution time', ['agent_name'])

# --- Job Queue Configuration ---
JOB_QUEUE_PREFIX = "job_queue"
JOB_QUEUE_PRIORITIES: Tuple[str, ...] = ("urgent", "high", "normal", "low")


def get_queue_key(priority: str) -> str:
    """Generate the Redis queue key for a given priority."""
    return f"{JOB_QUEUE_PREFIX}:{priority}"

# --- Security ---
security = HTTPBearer()

class TokenData(BaseModel):
    """JWT token payload"""
    user_id: str
    exp: datetime
    scopes: List[str] = []

    @field_validator('exp', mode='after')
    @classmethod
    def ensure_timezone(cls, value: datetime) -> datetime:
        """Normalize expiration timestamps to timezone-aware UTC datetimes."""

        if value.tzinfo is None:
            return value.replace(tzinfo=timezone.utc)
        return value.astimezone(timezone.utc)

async def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)) -> TokenData:
    """Verify and decode JWT token"""
    settings = get_settings()
    try:
        payload = jwt.decode(
            credentials.credentials,
            settings.secret_key.get_secret_value(),
            algorithms=[settings.jwt_algorithm]
        )
        token_data = TokenData(**payload)
        if token_data.exp < datetime.now(timezone.utc):
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token has expired"
            )
        return token_data
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials"
        )

def create_access_token(user_id: str, scopes: List[str] = []) -> str:
    """Create JWT access token"""
    settings = get_settings()
    expire = datetime.now(timezone.utc) + timedelta(hours=settings.jwt_expiration_hours)

    payload = {
        "user_id": user_id,
        "exp": expire,
        "scopes": scopes,
        "jti": secrets.token_urlsafe(16)  # JWT ID for revocation
    }

    return jwt.encode(
        payload,
        settings.secret_key.get_secret_value(),
        algorithm=settings.jwt_algorithm
    )

# --- Request/Response Models ---
class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    version: str
    timestamp: datetime
    database: str
    redis: str
    jobs_pending: int = 0

class HyperPromptRequest(BaseModel):
    """Validated hyper prompt request"""
    idea: str = Field(..., min_length=1, max_length=5000)
    agents: List[str] = Field(
        default_factory=lambda: ["Critic", "Coder", "User_Advocate"],
        max_length=10
    )
    priority: str = Field(default="normal", regex="^(low|normal|high|urgent)$")
    metadata: Dict[str, Any] = Field(default_factory=dict)

    @field_validator('agents')
    def validate_agents(cls, v):
        allowed_agents = {"Critic", "Coder", "User_Advocate", "Director", "Analyst", "Designer"}
        invalid = set(v) - allowed_agents
        if invalid:
            raise ValueError(f"Invalid agents: {invalid}")
        return v

class JobResponse(BaseModel):
    """Job status response"""
    job_id: str
    status: str
    created_at: datetime
    progress: int = 0
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

# --- Application State ---
class ApplicationState:
    """Centralized application state management"""

    def __init__(self):
        self.redis_pool: Optional[aioredis.ConnectionPool] = None
        self.redis_client: Optional[aioredis.Redis] = None
        self.db_engine = None
        self.db_session_factory = None
        self.shutdown_event = asyncio.Event()
        self.background_tasks = set()

    async def initialize(self, settings: Settings):
        """Initialize application resources"""
        # Redis connection pool
        self.redis_pool = aioredis.ConnectionPool.from_url(
            settings.redis_url,
            max_connections=settings.redis_pool_size,
            decode_responses=settings.redis_decode_responses
        )
        self.redis_client = aioredis.Redis(connection_pool=self.redis_pool)

        # Database engine
        self.db_engine = create_async_engine(
            settings.database_url,
            pool_size=settings.database_pool_size,
            max_overflow=settings.database_max_overflow,
            pool_pre_ping=True,
            echo=settings.debug
        )

        # Create tables
        async with self.db_engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)

        # Session factory
        self.db_session_factory = sessionmaker(
            self.db_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )

    async def cleanup(self):
        """Cleanup resources on shutdown"""
        self.shutdown_event.set()

        # Cancel background tasks
        for task in self.background_tasks:
            task.cancel()

        # Close connections
        if self.redis_pool:
            await self.redis_pool.disconnect()

        if self.db_engine:
            await self.db_engine.dispose()

app_state = ApplicationState()

# --- Circuit Breaker for External Services ---
AGENT_SYSTEM_MESSAGES = {
    "Critic": "You are the Critic agent. Provide rigorous critique and highlight potential risks.",
    "Coder": "You are the Coder agent. Produce implementation guidance and concise sample code when appropriate.",
    "User_Advocate": "You are the User Advocate agent. Emphasize user impact, accessibility, and clarity.",
    "Director": "You are the Director agent. Coordinate the team's efforts and outline next steps.",
    "Analyst": "You are the Analyst agent. Break down complex problems into actionable insights.",
    "Designer": "You are the Designer agent. Focus on usability, experience, and presentation.",
}
DEFAULT_AGENT_SYSTEM_MESSAGE = "You are a helpful AI teammate collaborating on complex tasks."

@circuit(failure_threshold=5, recovery_timeout=60)
async def call_external_agent(agent_name: str, prompt: Any, model: Optional[str] = None) -> Dict:
    """Call external AI agent with circuit breaker protection"""
    settings = get_settings()

    resolved_model = model or settings.default_model

    if isinstance(prompt, str):
        normalized_prompt = prompt
    elif prompt is None:
        normalized_prompt = ""
    else:
        try:
            normalized_prompt = json.dumps(prompt, sort_keys=True, default=str)
        except TypeError:
            normalized_prompt = str(prompt)

    cache_key = (
        f"agent:{agent_name}:{resolved_model}:"
        f"{hashlib.md5(normalized_prompt.encode('utf-8')).hexdigest()}"
    )

    redis_client: Optional[aioredis.Redis] = app_state.redis_client
    temporary_client: Optional[aioredis.Redis] = None

    if redis_client is None:
        temporary_client = aioredis.Redis.from_url(
            settings.redis_url,
            decode_responses=settings.redis_decode_responses
        )
        redis_client = temporary_client

    async def cache_result(result: Dict[str, Any]) -> None:
        try:
            serialized = json.dumps(result)
        except (TypeError, ValueError) as serialization_error:
            logging.exception(
                "Failed to serialize agent result for cache key %s", cache_key
            )
            raise serialization_error

        max_attempts = 3
        delay = 0.5

        for attempt in range(1, max_attempts + 1):
            try:
                await redis_client.setex(cache_key, 3600, serialized)
                return
            except Exception as cache_error:
                logging.warning(
                    "Attempt %d to cache agent result for key %s failed: %s",
                    attempt,
                    cache_key,
                    cache_error
                )
                if attempt == max_attempts:
                    logging.error(
                        "Exceeded retry attempts caching agent result for key %s",
                        cache_key
                    )
                    return
                await asyncio.sleep(delay)
                delay *= 2

    async def _close_temporary_client() -> None:
        if temporary_client is None:
            return

        # Try async close methods first so we don't leave connections dangling.
        for method_name in ("aclose", "close"):
            close_method = getattr(temporary_client, method_name, None)
            if close_method is None:
                continue

            try:
                result = close_method()
                if inspect.isawaitable(result):
                    await result
                return
            except Exception:
                logging.debug(
                    "Failed to close temporary Redis client using %s.",
                    method_name,
                    exc_info=True,
                )

        # Fall back to disconnecting the connection pool if available.
        pool = getattr(temporary_client, "connection_pool", None)
        disconnect = getattr(pool, "disconnect", None) if pool is not None else None
        if disconnect is None:
            return

        try:
            result = disconnect()
            if inspect.isawaitable(result):
                await result
        except Exception:
            logging.debug(
                "Failed to disconnect temporary Redis client's pool.",
                exc_info=True,
            )

    try:
        try:
            cached = await redis_client.get(cache_key)
        except Exception as cache_error:
            logging.warning(
                "Failed to fetch cached agent result for key %s: %s",
                cache_key,
                cache_error,
            )
            cached = None

        if cached:
            try:
                return json.loads(cached)
            except (TypeError, ValueError) as decode_error:
                logging.warning(
                    "Cached agent result for key %s is invalid JSON: %s",
                    cache_key,
                    decode_error,
                )

        system_message = AGENT_SYSTEM_MESSAGES.get(agent_name, DEFAULT_AGENT_SYSTEM_MESSAGE)
        try:
            _, response_text = await asyncio.to_thread(
                agents.run_agent,
                agent_name,
                system_message,
                normalized_prompt,
                resolved_model,
            )
        except agents.AgentExecutionError as agent_error:
            logging.error(
                "Agent '%s' execution failed: %s",
                agent_name,
                agent_error,
            )
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail=str(agent_error),
            ) from agent_error

        result = {
            "agent": agent_name,
            "response": response_text,
            "model": resolved_model,
        }

        await cache_result(result)
        return result
    finally:
        await _close_temporary_client()

# --- Background Tasks ---
async def process_job_queue():
    """Background job processor"""
    while not app_state.shutdown_event.is_set():
        try:
            job = None

            # Pop job from the highest priority queue with work
            for priority in JOB_QUEUE_PRIORITIES:
                queue_key = get_queue_key(priority)
                job_data = await app_state.redis_client.lpop(queue_key)
                if job_data:
                    job = json.loads(job_data)
                    break

            if job:
                await process_job(job)
            else:
                await asyncio.sleep(1)
        except Exception as e:
            logging.error(f"Job processor error: {e}")
            await asyncio.sleep(5)

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def process_job(job: Dict):
    """Process a single job with retry logic"""
    async with app_state.db_session_factory() as session:
        job_id = job['id']
        processing_time = datetime.now(timezone.utc)

        # Update job status
        await session.execute(
            update(Job)
            .where(Job.id == job_id)
            .values(status='processing', updated_at=processing_time, error=None)
        )
        await session.commit()

        try:
            # Process job (implement actual logic)
            result = await execute_agent_pipeline(job)

            # Update job completion
            completion_time = datetime.now(timezone.utc)

            await session.execute(
                update(Job)
                .where(Job.id == job_id)
                .values(
                    status='completed',
                    result=result,
                    completed_at=completion_time,
                    updated_at=completion_time,
                    error=None
                )
            )
            job_counter.labels(status='completed').inc()
        except Exception as e:
            # Update job failure
            failure_time = datetime.now(timezone.utc)

            await session.execute(
                update(Job)
                .where(Job.id == job_id)
                .values(
                    status='failed',
                    error=str(e),
                    updated_at=failure_time,
                    result=None,
                    completed_at=None
                )
            )
            job_counter.labels(status='failed').inc()
            raise
        finally:
            await session.commit()

async def execute_agent_pipeline(job: Dict) -> Dict:
    """Execute multi-agent pipeline"""
    agents = job.get('agents', ['Critic', 'Coder', 'User_Advocate'])
    prompt = job.get('prompt', '')

    results = {}
    for agent in agents:
        with agent_execution_time.labels(agent_name=agent).time():
            results[agent] = await call_external_agent(agent, prompt)

    return {
        'job_id': job['id'],
        'agent_results': results,
        'synthesis': 'Combined analysis from all agents',
        'timestamp': datetime.now(timezone.utc).isoformat()
    }

# --- Application Lifecycle ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle"""
    settings = get_settings()

    # Configure logging
    logging.basicConfig(
        level=getattr(logging, settings.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Initialize Sentry
    if settings.sentry_dsn:
        sentry_sdk.init(
            dsn=settings.sentry_dsn,
            environment=settings.environment,
            traces_sample_rate=0.1
        )

    # Initialize application state
    await app_state.initialize(settings)

    # Start background tasks
    task = asyncio.create_task(process_job_queue())
    app_state.background_tasks.add(task)

    logging.info(f"{settings.app_name} v{settings.app_version} started")

    yield

    # Cleanup
    await app_state.cleanup()
    logging.info("Application shutdown complete")

# --- FastAPI Application ---
app = FastAPI(
    title="ReVerse Enterprise",
    description="Production-ready multi-agent orchestration platform",
    version="4.0.0",
    lifespan=lifespan,
    docs_url="/api/docs" if get_settings().debug else None,
    redoc_url="/api/redoc" if get_settings().debug else None
)

# --- Middleware ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=get_settings().cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(TrustedHostMiddleware, allowed_hosts=get_settings().allowed_hosts)

if get_settings().sentry_dsn:
    app.add_middleware(SentryAsgiMiddleware)

# --- Exception Handlers ---
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Handle validation errors"""
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={
            "error": "Validation Error",
            "details": exc.errors(),
            "body": exc.body
        }
    )

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code
        }
    )

# --- Socket.IO Integration ---
sio = socketio.AsyncServer(
    async_mode='asgi',
    cors_allowed_origins=get_settings().cors_origins,
    logger=get_settings().debug,
    engineio_logger=False,
    ping_timeout=60,
    ping_interval=25
)

socket_app = socketio.ASGIApp(sio, app)

@sio.event
async def connect(sid, environ):
    """Handle WebSocket connection"""
    active_connections.inc()
    await sio.emit('connected', {'sid': sid}, room=sid)
    logging.info(f"Client connected: {sid}")

@sio.event
async def disconnect(sid):
    """Handle WebSocket disconnection"""
    active_connections.dec()
    logging.info(f"Client disconnected: {sid}")

# --- API Endpoints ---
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint for monitoring"""
    settings = get_settings()

    # Check database
    db_status = "healthy"
    try:
        async with app_state.db_session_factory() as session:
            await session.execute("SELECT 1")
    except:
        db_status = "unhealthy"

    # Check Redis
    redis_status = "healthy"
    try:
        await app_state.redis_client.ping()
    except:
        redis_status = "unhealthy"

    # Get pending jobs count across all priority queues
    jobs_pending = 0
    for priority in JOB_QUEUE_PRIORITIES:
        queue_key = get_queue_key(priority)
        jobs_pending += await app_state.redis_client.llen(queue_key)

    return HealthResponse(
        status="healthy" if db_status == "healthy" and redis_status == "healthy" else "degraded",
        version=settings.app_version,
        timestamp=datetime.now(timezone.utc),
        database=db_status,
        redis=redis_status,
        jobs_pending=jobs_pending
    )

@app.post("/api/v1/jobs", response_model=JobResponse, status_code=status.HTTP_201_CREATED)
async def create_job(
    request: HyperPromptRequest,
    background_tasks: BackgroundTasks,
    token: TokenData = Depends(verify_token)
):
    """Create a new job"""
    job_id = secrets.token_urlsafe(16)

    job = {
        "id": job_id,
        "user_id": token.user_id,
        "prompt": request.idea,
        "agents": request.agents,
        "priority": request.priority,
        "metadata": request.metadata,
        "created_at": datetime.now(timezone.utc).isoformat()
    }

    # Store in database
    async with app_state.db_session_factory() as session:
        new_job = Job(
            id=job_id,
            user_id=token.user_id,
            payload=job,
            status="pending"
        )
        session.add(new_job)
        await session.commit()

    # Queue for processing
    queue_key = get_queue_key(request.priority)
    await app_state.redis_client.rpush(queue_key, json.dumps(job))
    job_counter.labels(status='queued').inc()

    request_counter.labels(method='POST', endpoint='/api/v1/jobs', status=201).inc()

    return JobResponse(
        job_id=job_id,
        status="pending",
        created_at=datetime.now(timezone.utc)
    )

@app.get("/api/v1/jobs/{job_id}", response_model=JobResponse)
async def get_job_status(
    job_id: str,
    token: TokenData = Depends(verify_token)
):
    """Get job status"""
    async with app_state.db_session_factory() as session:
        stmt = select(Job).where(
            Job.id == job_id,
            Job.user_id == token.user_id
        )
        result = await session.execute(stmt)
        job = result.scalar_one_or_none()

        if not job:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Job not found"
            )

        return JobResponse(
            job_id=job.id,
            status=job.status,
            created_at=job.created_at,
            result=job.result,
            error=job.error
        )

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return generate_latest()

# --- Main Entry Point ---
if __name__ == "__main__":
    settings = get_settings()

    uvicorn.run(
        "src.reverse.main_enterprise:socket_app" if settings.environment == "production" else "src.reverse.main_enterprise:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.debug,
        log_level=settings.log_level.lower(),
        access_log=settings.debug,
        workers=4 if settings.environment == "production" else 1
    )