# -*- coding: utf-8 -*-
"""src/reverse/agents.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G_jlzLvZZKCWKhzXetrV81lx2I1Hc39E
"""

"""Utility helpers for executing Ollama-backed agents."""

from __future__ import annotations

import logging
import ollama
import inspect
import asyncio


class AgentExecutionError(RuntimeError):
    """Raised when an agent invocation fails to produce a response."""


_DEFAULT_MODEL_FALLBACK = "phi3:mini"
logger = logging.getLogger(__name__)


def _determine_model(preferred_model: str | None) -> str:
    """Choose the Ollama model to use for the current invocation."""

    if preferred_model:
        return preferred_model

    try:
        # Import locally to avoid circular imports during module initialization.
        from src.reverse.main_enterprise import get_settings  # type: ignore import-not-found

        settings = get_settings()
        configured = getattr(settings, "default_model", None)
        if isinstance(configured, str) and configured.strip():
            return configured
    except Exception:
        logger.debug(
            "Falling back to default Ollama model '%s' due to missing settings.",
            _DEFAULT_MODEL_FALLBACK,
            exc_info=True,
        )

    return _DEFAULT_MODEL_FALLBACK


def run_agent(
    agent_name: str,
    system_message: str,
    user_prompt: str,
    model: str | None = None,
) -> tuple[str, str]:
    """Execute an agent against Ollama and return the response content.

    Parameters
    ----------
    agent_name:
        Identifier used for logging and reporting.
    system_message:
        Instructional message that sets context for the agent.
    user_prompt:
        The concrete user question or directive.
    model:
        Optional Ollama model identifier. When omitted the application's
        configured default model is used, falling back to ``phi3:mini`` if the
        settings module cannot be loaded.
    """

    resolved_model = _determine_model(model)
    logger.info("-> Agent '%s' is thinking with model '%s'...", agent_name, resolved_model)
    try:
        response = ollama.chat(
            model=resolved_model,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_prompt},
            ],
        )
        logger.info("<- Agent '%s' finished.", agent_name)
        return (agent_name, response["message"]["content"])
    except Exception as exc:
        logger.error("<- Agent '%s' failed.", agent_name, exc_info=True)
        raise AgentExecutionError(
            f"Agent '{agent_name}' failed to respond."
        ) from exc